{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebe86c0-44bf-4333-86af-c0b5f4083b90",
   "metadata": {},
   "source": [
    "# Basic LangChain concepts\n",
    "In this notebook we will explore LangChain framework capabilities and use it to interact with a LLM. You can either install [Ollama](https://ollama.com/) on your computer, use one of the models provided by [HuggingFace](https://python.langchain.com/docs/integrations/llms/huggingface_pipelines) or create an [OpenAI API token](https://openai.com/). Remember to install the requirements from the `requirements.txt` file using `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9747ad-a093-410e-868d-8b1a18b891e2",
   "metadata": {},
   "source": [
    "## Basic LLM interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb9531-ffd5-4330-8b9a-cb321e5f2ad1",
   "metadata": {},
   "source": [
    "### Model loading\n",
    "Loading a model and using it with LangChain is extremely easy. All models implement [Runnable interface](https://python.langchain.com/docs/integrations/llms/) which makes it easy to replace them in the chain. Define your selected model and proceed to the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c9080-3f86-4248-9f4d-f2f8d8bfe916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ollama model. Requires to install the Ollama package: https://ollama.com/\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758d9dc-f6e7-4ba8-a4e0-769456dcb956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HuggingFace model. Requires to install extra dependencies with `pip install transformers sentence_transformers torch`\n",
    "# from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# hf = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"gpt2\",\n",
    "#     task=\"text-generation\",\n",
    "#     pipeline_kwargs={\"max_new_tokens\": 10},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e962b-638b-4bb0-ab88-0c4fb3b9f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenAI model. Requires API keys and extra dependencies `pip install langchain-openai`\n",
    "# %export OPENAI_API_KEY=\"...\"\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c8263-564d-499f-90f2-44a92e94d308",
   "metadata": {},
   "source": [
    "To query the model you can simply run the `invoke` method. Experiment with different prompts to get an intuition about the model behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d06e1-42fa-4a93-b3a5-cfd82ad2a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"Hey, nice to meet you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db81cc3e-dacc-4bde-a29e-b22ebe80e366",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "LangChain allows to define a prompt template that will be reused to query a model. It is a very convenient mechanism that allows you to deliver instructions and extra context to the model. You can experiment with system messages to influence the behavior of the model and assign it different roles. Try a couple of system prompts to see what you can do with it.\n",
    "\n",
    "A great feature of LangChain is its [expression language](https://python.langchain.com/docs/expression_language/get_started). It allows defining chains of operations that will be performed sequentially. Here we can connect a prompt template with an LLM such that we don't have to provide repeatable content of the query over and over again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d3fe8-54d4-4eec-b275-7da02cdf8f19",
   "metadata": {},
   "source": [
    "#### Simple prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d412f26-2aa0-4ad6-9a61-49d7719548e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the following question: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.invoke(\"How are you doing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e47ae6-5086-444b-ade0-b9e2a99f10a6",
   "metadata": {},
   "source": [
    "**Exercise:** create a prompt template that includes `context` and `question` fields and run it using the `invoke` method. To achieve that you will have to run the method with a dictionary as an input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ee792-b88d-4975-a3ff-fa6601e5f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "### TODO: create prompt template with `context` and `question` fields ###\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.invoke(### TODO: invoke template with proper parameters ###)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91596fb-989d-41da-a676-0a712eefef25",
   "metadata": {},
   "source": [
    "#### Chaining prompt and model\n",
    "To build a chain of operations you can simply use `|` pipe operator. Now you can chain your prompt with the llm to get an answer to your question given the context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf2856d-9dea-46c7-a7a2-86e1302f6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da4b2c-9618-452a-9be0-582cf3c53c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = chain.invoke(\n",
    "    ### TODO: Invoke the chain with proper parameters ###\n",
    ")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f50b1-bc1c-4c78-9c30-37047334bac3",
   "metadata": {},
   "source": [
    "#### Chat prompts\n",
    "Chat prompts may include extra instructions like system messages about a role the agent is playing. This message will affect the model behavior. It is a place where you can put additional security guards ie. forbidden actions. You can also influence the model to behave in a particular way.\n",
    "\n",
    "**Exercise:** Experiment with the system message and see how the model replies change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4a56e-95d9-4605-8f2f-572fe5aeb9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class agent and you serve your customers well\"),\n",
    "    (\"user\", \"Here is my question: {input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm \n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86764b0-6d28-4193-89a9-337687c2d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke(\"I want to cancel my order\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967327df-8698-4b4b-b284-7aafb870db76",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "For semantic search we need to create embeddings for our documents. In this section we will experiment with embeddings of small portions of text to get a better understanding of how similarity works. LangChain provides a great interface for [embedding models](https://python.langchain.com/docs/modules/data_connection/text_embedding/) and integrates smoothly with Ollama, OpenAI or HuggingFace sentence transformers. \n",
    "\n",
    "**Exercise:** Create embeddings of your sentences and check what is the similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015b054-63cc-465e-80fd-9f010bce61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171dae8e-a13c-4afd-b93d-869c4991eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Embeddings, requires OpenAI API key. \n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04d4cf-07d0-4396-8bba-1a01bd646999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09417a98-d890-469b-b0fb-bf90ad70c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vector1: list, vector2: list) -> float:\n",
    "    # Convert lists to NumPy arrays\n",
    "    v1 = np.array(vector1)\n",
    "    v2 = np.array(vector2)\n",
    "    \n",
    "    # Compute the dot product between the two vectors\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    \n",
    "    # Compute the L2 norms (Euclidean norms) of each vector\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    \n",
    "    # Calculate the cosine similarity\n",
    "    return np.dot(v1,v2)/(norm_v1*norm_v2)\n",
    "\n",
    "    \n",
    "# For short sentences it is easy to introduce some noise. \n",
    "# Ie. model assigns big similarity to sentences that ends with a dot.\n",
    "query1 = embeddings.embed_query(\"My package was not delivered\")\n",
    "query2 = embeddings.embed_query(\"My package was lost\")\n",
    "query3 = embeddings.embed_query(\"I want to cancel my order\")\n",
    "\n",
    "print(\"Package not delivered & package lost:\", cosine_similarity(query1, query2))\n",
    "print(\"Package not delivered & order cancellation:\", cosine_similarity(query1, query3))\n",
    "print(\"Package lost & order cancellation:\", cosine_similarity(query2, query3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7493ae-5cfa-4fd4-8c7c-de1b34ad8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with the similarity score\n",
    "sentence1 = \"### TODO ###\"\n",
    "sentence2 = \"### TODO ###\"\n",
    "cosine_similarity(embeddings.embed_query(sentence1), embeddings.embed_query(sentence2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774c577-49c4-4289-8485-ae8ff4a56bdf",
   "metadata": {},
   "source": [
    "#### (Optional) Visualize the embeddings\n",
    "Even though embeddings are multidimensional vectors it is possible to visualize them on a 2d plane thanks to dimensionality reduction techniques. Below I used PCA from sklearn library to reduce the vectors to 2d space and plot them using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39e7df-8e4d-4c92-9d38-99288552e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn matplotlib -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbdc7e-5303-45ad-a6aa-9cedc8487f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collection of texts to compare embeddings. \n",
    "# Add your texts to the list.\n",
    "documents = [\n",
    "    \"My package was not delivered\",\n",
    "    \"My package was lost\",\n",
    "    \"I want to cancel my order\",\n",
    "    \"Today is a very beautiful day\",\n",
    "    \"The weather is great\"\n",
    "]\n",
    "# Compute embeddings using the embedding model. \n",
    "vectors = embeddings.embed_documents(documents)\n",
    "# Dimensionality reduction, allows to get from multidimentional space to 2d.\n",
    "reduced = PCA(n_components=2).fit_transform(vectors)\n",
    "# Visualize the results in 2d.\n",
    "scatter = plt.scatter(reduced[:,0], reduced[:,1], c=list(range(len(documents))))\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=documents)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
