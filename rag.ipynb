{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56969b09-4a94-40fe-84a5-1f23e44734ed",
   "metadata": {},
   "source": [
    "# RAG implementation using LangChain\n",
    "In this notebook we will explore a simple RAG pipeline using LangChain framework. Having the knowledge on basic concepts that you gained from the intro notebook you should be able to fill in the gaps and run your first RAG pipeline. If you want to explore more check out the LangChain documentation with a [RAG Q&A example](https://python.langchain.com/docs/use_cases/question_answering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf303c6-56d2-420d-8996-efb5d92c241a",
   "metadata": {},
   "source": [
    "## Loading documents\n",
    "First, we need to load the relevant knowledge documents so the model can refer to them while answering the questions. I have prepared a couple of files with customer support policies that are located inside the `policies` directory. LangChain has a large number of document loaders available, for example you can load content of websites and remote storages. For more details refer to [documentation](https://python.langchain.com/docs/modules/data_connection/document_loaders/).\n",
    "\n",
    "In this exercise we are going to use `DirectoryLoader` that parses the directory for files and uses `UnstructuredLoader` to load textual data. Files are found using the pattern matching for `txt` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a287c90-5703-45cd-b373-fa6b476f418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    'policies', \n",
    "    glob=\"*.txt\", \n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"Documents loaded:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f214e68-d7fc-41d9-a27d-96a02d8ae282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview content of a document\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e63aa4-be36-4670-9063-9cbdf2c72257",
   "metadata": {},
   "source": [
    "## Splitting documents\n",
    "It is especially desirable to retrieve knowledge from enormous knowledge bases that are hard to traverse by humans. For example, imagine thousands of pages of legal documentation. Reading it would take long days for a single person. One of the limitations of LLMs are limited context windows which comes from the quadratic complexity of the [transformer attention layer](https://nlp.seas.harvard.edu/2018/04/03/attention.html). Because of that, long documents should be split into smaller, meaningful chunks of text. The split can't be done randomly, it would break the meaning of sentences and may cause loss of information. Thankfully, LangChain delivers a library of [text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) that you can use. In this exercise the policies are relatively short and can easily fit the context window. The default text splitter will leave them undivided. However, you can experiment with the `chunk_size` to see how the splitter slices the document into meaningful chunks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ba022-eab5-46b9-830a-dc77aea22b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split texts into chunks. Our documents are quite short so they won't be split. \n",
    "# To experiment with different settings uncomment the arguments to override default settings.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # chunk_size=1000,\n",
    "    # chunk_overlap=20,\n",
    "    # length_function=len,\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "print(\"Number of chunks:\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf821eb-da04-4c39-8331-97ac4a5c9993",
   "metadata": {},
   "source": [
    "## Initialize vector store\n",
    "There are a number of vector databases supported by LangChain, ranging from Sklearn implementation to cloud based databases. For the full list of integrations refer to [documentation](https://python.langchain.com/docs/integrations/vectorstores). Here we are going to use [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss) - Facebook AI Similarity Search, which is easy to install using python package manager. Create a vector store by passing documents and embedding models to the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b1e33-15d5-4255-9e8a-a46843aa63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = ### TODO: create embedding model ###\n",
    "\n",
    "vector_store = FAISS.from_documents(### TODO: provide documetns and embedding model ###)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b722dc30-6a8c-47c1-9f1f-c86a536ee5e1",
   "metadata": {},
   "source": [
    "Vector store provides a method for similarity search out of the box. It is very easy to retrieve related documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fcf71f-2db7-4466-b956-3aab776a043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = vector_store.similarity_search(\"I received wrong size of the item\")\n",
    "print(\"Retrieved documents:\", len(retrieved))\n",
    "print(\"Document content:\", retrieved[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10580af-1cc5-479c-9aff-717b11bd8b07",
   "metadata": {},
   "source": [
    "## RAG pipeline\n",
    "Having all the pieces of the pipeline we can create a chain that takes a question and answers it given the knowledge from the policies. In the previous notebook you learned how to assemble components into a pipeline using the pipe operator `|`. Here we are going to use [helper functions](https://python.langchain.com/docs/modules/chains) provided by LangChain to compose complex RAG chain. \n",
    "\n",
    "- [`create_stuff_documents_chain`](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html#langchain.chains.combine_documents.stuff.create_stuff_documents_chain): This chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.\n",
    "- [`create_retrieval_chain`](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html#langchain.chains.retrieval.create_retrieval_chain): This chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a response.\n",
    "\n",
    "Your chat prompt template should take `{context}` and `{input}` fields. Having that, you can chain the prompt and the llm using the `create_stuff_documents_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da165d-9544-461d-9ac8-a6950e019ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "llm = ### TODO: Create a model ###\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "   ### TODO: create prompt template with `context` and `input` fields ###\n",
    "\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(### TODO: provide llm and prompt ###)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa5c47-f2b7-497f-bb73-1c899b0875f8",
   "metadata": {},
   "source": [
    "Next, we will chain together the retriever (which is simply a wrapper around the vector store) and the combined document chain that you created above. It will make a chain that is able to retrieve relevant documents from the vector store and give the output for a given query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cbbd63-1775-4bcd-9fe9-71d4837faf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(### TODO: provide retriever and document chain ###)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba180e7-572d-4460-a0af-8b6971042efc",
   "metadata": {},
   "source": [
    "## Running the chain\n",
    "The final chain implements a runnable interface as well. All you need to do is to provide your question as an input.\n",
    "\n",
    "Some of the questions you can ask: \n",
    "- Accepted methods of payments\n",
    "- Customer was charged twice\n",
    "- Package was lost\n",
    "- Order cancellation\n",
    "- Item arrived damaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6f99fa-993e-4683-b9e4-7627e05a4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"### TODO: Insert your question ###\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d52d2-172b-4b64-9c8b-0e9bca1de93a",
   "metadata": {},
   "source": [
    "## Further work\n",
    "- Check if LLM is willing to give away your company secrets, ask it to tell something confidential\n",
    "- Try using system prompt from the intro notebook to prevent model from going astray and perform only allowed actions - `ChatPromptTemplate.from_messages`\n",
    "- To further improve the pipeline you can implement [memory mechanism](https://python.langchain.com/docs/use_cases/question_answering/chat_history) that holds previous conversation so you can ask follow-up questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83d248-d57e-403c-aad4-a6325d0f39ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
